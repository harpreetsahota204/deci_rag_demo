{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efd2d32-2f7f-47d8-9e07-04e3b132d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a20628-28da-405e-8e5d-2256793f224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132016c6-fd31-4a8e-908a-91e95cd5754c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1437\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[0;34m(cls, path)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '../src'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcleaning_utils\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Tuple, Dict\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1473\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[0;34m(cls, path)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1415\u001b[0m, in \u001b[0;36m_path_hooks\u001b[0;34m(path)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1700\u001b[0m, in \u001b[0;36mpath_hook_for_FileFinder\u001b[0;34m(path)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1571\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, path, *loader_details)\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import cleaning_utils\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from llama_index.core import  Document\n",
    "\n",
    "from llama_index.core.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    SummaryExtractor,\n",
    "    KeywordExtractor\n",
    "    \n",
    ")\n",
    "\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "\n",
    "from llama_index.core.node_parser import TokenTextSplitter, SentenceWindowNodeParser, SemanticSplitterNodeParser, SentenceSplitter\n",
    "from llama_index.core.schema import BaseNode, TextNode\n",
    "\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aded5a-6c06-4bb0-b9fe-404dc6948126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_clean_text(cleaned_texts: List[Tuple[str, Dict]]) -> List[Document]:\n",
    "    documents = [Document(text=t, \n",
    "                          metadata=m, \n",
    "                          metadata_seperator=\"\\n\\n\", \n",
    "                          excluded_llm_metadata_keys=[\"file_name\",\n",
    "                                                      \"publication_date\", \n",
    "                                                      \"referenced_websites\", \n",
    "                                                      \"section_summary\", \n",
    "                                                      \"excerpt_keywords\",\n",
    "                                                      \"questions_this_excerpt_can_answer\"\n",
    "                                                     ]\n",
    "                         ) for (t, m) in cleaned_texts]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0054c-3788-4298-a485-41c7aed9c031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in atexit callback: <bound method InteractiveShell.atexit_operations of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f0016e40090>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/demotime/miniconda3/envs/decilm_rag/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3920, in atexit_operations\n",
      "    self._atexit_once()\n",
      "  File \"/home/demotime/miniconda3/envs/decilm_rag/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3899, in _atexit_once\n",
      "    self.reset(new_session=False)\n",
      "  File \"/home/demotime/miniconda3/envs/decilm_rag/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 1399, in reset\n",
      "    self.history_manager.reset(new_session)\n",
      "  File \"/home/demotime/miniconda3/envs/decilm_rag/lib/python3.11/site-packages/IPython/core/history.py\", line 607, in reset\n",
      "    self.dir_hist[:] = [Path.cwd()]\n",
      "                        ^^^^^^^^^^\n",
      "  File \"/home/demotime/miniconda3/envs/decilm_rag/lib/python3.11/pathlib.py\", line 907, in cwd\n",
      "    return cls(os.getcwd())\n",
      "               ^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "cleaned_pdfs = cleaning_utils.clean_and_prepare_texts('../SuperMicro_Solution_Brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e78bf-0914-4cbd-a9ff-4648256efc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents_from_clean_text(cleaned_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e207bd-a530-4833-93d9-5fee27cc6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e791668-d66e-4eb7-9266-d6d35e2d1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    generate_kwargs={\"temperature\": 0.25, \n",
    "                     \"do_sample\": True, \n",
    "                     \"top_p\":0.80\n",
    "                     },\n",
    "    is_chat_model=True,\n",
    "    system_prompt = \"You are an AI assistant that follows instructions extremely well. Help as much as you can.\",\n",
    "    # query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Deci/DeciLM-7B-instruct\",\n",
    "    model_name=\"Deci/DeciLM-7B-instruct\",\n",
    "    device_map=\"xpu\",\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": \"auto\",\n",
    "                  \"trust_remote_code\":True\n",
    "                 },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dc54b-e121-4033-8543-cab56544982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff1cc0-d033-4397-9d1b-7e172b78c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = \"\"\" Here is the context:\n",
    "{context_str}\n",
    "\n",
    "Given the contextual information, generate {num_questions} questions this context can provide \\\n",
    "specific answers about the products, software, hardware, and solutions mentioned in this document\\\n",
    "which are unlikely to be found elsewhere.\n",
    "\n",
    "Higher-level summaries of the surrounding context may be provided as well.  Try using these summaries to generate better questions that this context can answer.\"\"\"\n",
    "\n",
    "summary_prompt = \"\"\" Here is the content of the section:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Provide a Summary of key topics, entities, products, software, hardware, and solutions discussed in this section.\n",
    "\n",
    "Summary: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", \n",
    "    chunk_size=256, \n",
    "    chunk_overlap=8\n",
    ")\n",
    "\n",
    "qa_extractor = QuestionsAnsweredExtractor(\n",
    "    questions=5, \n",
    "    prompt_template=qa_prompt,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "summary = SummaryExtractor(\n",
    "    summaries = [\"self\"], \n",
    "    prompt_template=summary_prompt,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "key_words = KeywordExtractor(\n",
    "    keywords=5,\n",
    "    num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a42e1c-b5d1-4aa1-a65e-9a7c117bb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_docs = documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b5b00-a0e5-4dc0-9f49-f46a75e74df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, summary, key_words, qa_extractor]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=some_docs,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    "    # num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d17a3-22f1-4542-9c95-88734a251a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[2].__dict__['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a9cb1-ac81-4241-8708-c3ac79ae959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a31049-d0cb-41f3-ab86-76373a263ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import qdrant_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f3224-4486-473b-8c47-dab936252c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19cfd5-7700-41c0-9768-9efe3fe33251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"WhereIsAI/UAE-Large-V1\",\n",
    "    tokenizer_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2113cb-555c-4b76-b182-4de2ffcebf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc1c4d-1ce6-4cc5-aa20-7e0f4bf0dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # uri=\"http://<host>:<port>\"\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe2f21-fe9e-4e90-bd57-d2220f2f54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc1ade-14f4-4b21-8265-0d0a688ef889",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ff868-0fa8-4178-910a-4d7cfc7207f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decilm_rag",
   "language": "python",
   "name": "decilm_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
