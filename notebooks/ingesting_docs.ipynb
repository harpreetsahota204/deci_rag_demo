{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efd2d32-2f7f-47d8-9e07-04e3b132d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a20628-28da-405e-8e5d-2256793f224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "132016c6-fd31-4a8e-908a-91e95cd5754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demotime/miniconda3/envs/decilm_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cleaning_utils\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from llama_index.core import  Document\n",
    "\n",
    "from llama_index.core.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    SummaryExtractor,\n",
    "    KeywordExtractor\n",
    "    \n",
    ")\n",
    "\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "\n",
    "from llama_index.core.node_parser import TokenTextSplitter, SentenceWindowNodeParser, SemanticSplitterNodeParser, SentenceSplitter\n",
    "from llama_index.core.schema import BaseNode, TextNode\n",
    "\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8aded5a-6c06-4bb0-b9fe-404dc6948126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_clean_text(cleaned_texts: List[Tuple[str, Dict]]) -> List[Document]:\n",
    "    documents = [Document(text=t, \n",
    "                          metadata=m, \n",
    "                          metadata_seperator=\"\\n\\n\", \n",
    "                          excluded_llm_metadata_keys=[\"file_name\",\n",
    "                                                      \"publication_date\", \n",
    "                                                      \"referenced_websites\", \n",
    "                                                      \"section_summary\", \n",
    "                                                      \"excerpt_keywords\",\n",
    "                                                      \"questions_this_excerpt_can_answer\"\n",
    "                                                     ]\n",
    "                         ) for (t, m) in cleaned_texts]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e0054c-3788-4298-a485-41c7aed9c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "cleaned_pdfs = cleaning_utils.clean_and_prepare_texts('../SuperMicro_Solution_Brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4e78bf-0914-4cbd-a9ff-4648256efc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents_from_clean_text(cleaned_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e207bd-a530-4833-93d9-5fee27cc6047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': 'c87702b2-6ce7-4c10-8f40-dbdaf06c77c0',\n",
       " 'embedding': None,\n",
       " 'metadata': {'file_name': 'Solution-Brief_Workstations_Entertainment.pdf',\n",
       "  'publication_date': 'December 2021',\n",
       "  'referenced_websites': ['https://www.supermicro.com/en/products/superworkstation']},\n",
       " 'excluded_embed_metadata_keys': [],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'publication_date',\n",
       "  'referenced_websites',\n",
       "  'section_summary',\n",
       "  'excerpt_keywords',\n",
       "  'questions_this_excerpt_can_answer'],\n",
       " 'relationships': {},\n",
       " 'text': \"1 Supermicro Workstation Family Media and Entertainment is a broad and diverse industry where companies are required to work and collaborate seamlessly to succeed. The ability to accelerate production workflows and gain value faster are top goals for today’s media companies. To stay ahead of the competition, leaders in the industry are implementing cutting-edge workstations to modernize their work environments. Advancements in virtual production, rendering, simulation, and artificial intelligence (AI) continue to propel the future of entertainment. Next generation workstations are the ideal foundation to reinvent how content is created, distributed, and consumed. These platforms combine robust compute technology, CPU and GPU acceleration, more memory, increased storage, and comprehensive management software to unlock high performance levels. 1 Partnering for Success 2 Modernizing A State-of-the-Art Production Environment 3 Summary 5 As a global leader in high performance, high efficiency server technology and innovation, we develop and provide end-to-end green computing solutions to the data center, cloud computing, enterprise IT, big data, HPC, and embedded markets. Our Building Block Solutions approach allows us to provide a broad range of SKUs, and enables us to build and deliver application- optimized solutions based upon your requirements. 2 Next generation workstations are powering the most advanced, visually rich feature films and television shows ever created. The latest platforms are engineered to deliver unparalleled compute and graphics capabilities. As a result, companies across the industry are seeing dramatic improvements in how they operate, using applications such as computer-aided design (CAD) and 2D and 3D design and animation. Media companies can gain significant advantages by adopting accelerated workstations: • Achieving cinematic quality results in less time, even using complex datasets • Increasing photo-realistic rendering throughput to accelerate creative design-making • Eliminating tedious or repetitive tasks so that artists can focus on high- value work Moving forward, visual computing solutions will be crucial to empower teams of artists, editors, and directors across disparate locations. The ideal work environment will be able to run a wide variety of applications with speed and precision. Partnering for Success Supermicro and NVIDIA are pioneering the next generation of visual computing to empower creativity and innovation. Together, we provide the correct visual computing solutions to improve the quality and speed of any production. Supermicro is a global leader in high-performance, high-efficiency technology, offering the broadest product portfolio for robust workstations. The goal is to enable the success of all customers. Supermicro achieves this through extensive engineering expertise and the industry’s broadest product portfolio, which offers green computing technologies that reduce energy costs, effectively allocate resources to tackle complex media workflows and drive down operational costs. In partnership with NVIDIA, we offer a range of performance-boosting solutions to help media companies work better, smarter, and faster. We build IT environments that deliver the efficiency, acceleration, and reliability that media teams depend on to share rapidly, review, and edit content. Leveraging first-to-market innovations from Supermicro and NVIDIA RTX technology, these workstations are purpose-built for unprecedented rendering, graphics, AI, and performance at scale to enhance any application. These server-grade workstations are expertly designed to optimize any application so that companies can complete critical projects in record time. Our joint technologies prepare media teams with computing power to execute a broad range of complex tasks: • Create highly sophisticated models, characters, and environments in real-time • Iterate more and render faster with unprecedented performance • Collaborate in real-time across popular content creation applications Media production requires powerful compute and graphics capabilities wherever you need to work: • High throughput and low latency to power diverse applications • Increased operational performance and reliability • Rapid refresh rates and up to 8K screen resolution • Greater memory and bandwidth to boost productivity 3 • Power massive virtual production stages in extended reality (XR) Supermicro workstations based on NVIDIA RTX technology provide high throughput over the previous generation, dramatically accelerating production pipelines, enabling higher-fidelity workflows, from interactive rendering to real-time virtual production. These applications enable a range of tasks, such as advanced scene composition for animated content, allowing companies to interactively assemble, light, simulate, and render scenes in real-time. That’s why major movie studios, post houses, and VFX studios trust Supermicro and NVIDIA workstation solutions to deliver the reliability, performance, and scalability they need to succeed. Modernizing A State-of-the-Art Production Environment Supermicro offers a comprehensive portfolio of workstations to meet the escalating demand for diverse and high-quality content. Supermicro workstations are fast, reliable, and cost-effective, utilizing enterprise-grade technologies tested and validated to meet an organization's specific application requirements. Each platform features a wide range of industry standard components that can be optimally configured to fit needs— including NVMe storage, industry-leading CPUs, and the extreme processing power of NVIDIA RTX. Solutions from Supermicro and NVIDIA offer a high degree of flexibility and upgradability fueled by unprecedented performance to support the rising need for graphics and AI wherever an enterprise's media teams need to work. Supermicro workstations are assembled and tested at a production facility in the USA. For EMEA and APAC companies, Supermicro builds workstations at production facilities in the Netherlands and Taiwan. Now, media companies worldwide can configure and deploy the ideal workstation to accelerate their unique workloads. Workstations from Supermicro and NVIDIA deliver the right tools to help media companies thrive: • Optimal application performance: Dramatically accelerate production pipelines, from previsualization through final frames • Rich, expansive visual workspaces: Increase production value on set while enabling real-time iterations • Proven reliability and improved manageability: Enterprise-class IT manageability, including configuration, monitoring, and diagnostic tools, with both local and remote access 4 5039A-I 5049A-T 5014A-TT Single-processor workstations provide exceptional power to execute graphics-intensive workloads at scale Entry-level configurations are engineered to be cost-efficient and efficient to empower teams using 3D modeling and design applications. Workstations are fully configurable and provide greater acceleration with server-grade reliability Mainstream configurations with advanced graphics capabilities deliver unparalleled performance for rendering. Designed for critical speed and compute capacity to optimize the most data-intensive workloads at scale, all with an energy efficient power supply Expert configurations with robust visual computing capabilities are designed to optimize high-end applications such as larger-scale rendering. - Intel Xeon W-2200 processor, up to 18 cores - 64GB DDR4-2933 Memory - NVIDIA RTX A4500 - 1TB M.2 NVMe + 6TB HDD - Windows 10/11 Pro 64 - 2nd Gen Intel Xeon Scalable processor, up to 28 cores total - 128GB DDR4-3200 Memory NVIDIA RTX A4500 - 2TB M.2 NVMe + 4x 3.8TB SSD - Windows 10/11 Pro 64 - AMD Ryzen Threadripper PRO 3900WX processor, up to 64 cores - 512GB DDR4-3200 Memory NVIDIA RTX A6000 - 3x 2TB M.2 PCIe Gen 4 NVMe in RAID 5 + 15TB U.2 PCIe Gen 4 SSD - Windows 10/11 Pro 64 5 Summary Supermicro and NVIDIA empower the media and entertainment industry to work, collaborate, and create content with incredible speed and stunning quality. Some of the largest production houses have adopted state-of-the-art workstations to work better, smarter, and faster than ever before. With solutions that are purpose-built to deliver competitive price-performance as well as reliable security and flexibility, media companies can execute any project with confidence as they push the boundaries of today’s artistic creations. In addition, organizations can benefit from solutions and capabilities that are the best in the industry: • Best performance: Highest memory and storage capacities available in a single tower system, featuring up to four passively cooled GPUs in tower form factor. Supermicro is the only manufacturer to offer up to four NVIDIA A100 Tensor Core GPUs in multiple models, with up to 80 cores, 4TB of memory, 61.44TB of NVMe, and optional DCPMM support. • Best expandability Up to six PCIe Gen4 x16 expansion slots, or up to four PCIe Gen4 M.2 with optional hardware RAID 0/1/5/10 support. • Best component selection: Supermicro validates a wide variety of memory, storage, and networking components with different specifications to help configure an optimized system for demanding needs without locking into one brand. • Best assembly and local support: All workstation systems shipped in the Americas are built and tested at Supermicro headquarters in San Jose, California, and include technical support services by in-house Supermicro engineers and product managers. Whether the team is building intricate 2D and 3D models, rendering photo-realistic designs, or simulating scenes, Supermicro and NVIDIA have the best product selections and configurations to boost a team's productivity. These solutions create an environment that allows media companies to scale faster and deliver the ultimate application experience to artists, editors, and directors everywhere. Together, Supermicro and NVIDIA can help anyone deploy the ideal workstation to optimize for specific media projects, from intensive graphics work to the cutting-edge of AI. It’s time for organizations to disrupt the entertainment industry. Visit Supermicro online to learn how.\",\n",
       " 'start_char_idx': None,\n",
       " 'end_char_idx': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}',\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_seperator': '\\n\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e791668-d66e-4eb7-9266-d6d35e2d1f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|                                                                                                              | 0/3 [00:00<?, ?it/s]/home/demotime/miniconda3/envs/decilm_rag/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:13<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    generate_kwargs={\"temperature\": 0.25, \n",
    "                     \"do_sample\": True, \n",
    "                     \"top_p\":0.80\n",
    "                     },\n",
    "    is_chat_model=True,\n",
    "    system_prompt = \"You are an AI assistant that follows instructions extremely well. Help as much as you can.\",\n",
    "    # query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Deci/DeciLM-7B-instruct\",\n",
    "    model_name=\"Deci/DeciLM-7B-instruct\",\n",
    "    device_map=\"xpu\",\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": \"auto\",\n",
    "                  \"trust_remote_code\":True\n",
    "                 },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06dc54b-e121-4033-8543-cab56544982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ff1cc0-d033-4397-9d1b-7e172b78c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = \"\"\" Here is the context:\n",
    "{context_str}\n",
    "\n",
    "Given the contextual information, generate {num_questions} questions this context can provide \\\n",
    "specific answers about the products, software, hardware, and solutions mentioned in this document\\\n",
    "which are unlikely to be found elsewhere.\n",
    "\n",
    "Higher-level summaries of the surrounding context may be provided as well.  Try using these summaries to generate better questions that this context can answer.\"\"\"\n",
    "\n",
    "summary_prompt = \"\"\" Here is the content of the section:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Provide a Summary of key topics, entities, products, software, hardware, and solutions discussed in this section.\n",
    "\n",
    "Summary: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", \n",
    "    chunk_size=256, \n",
    "    chunk_overlap=8\n",
    ")\n",
    "\n",
    "qa_extractor = QuestionsAnsweredExtractor(\n",
    "    questions=5, \n",
    "    prompt_template=qa_prompt,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "summary = SummaryExtractor(\n",
    "    summaries = [\"self\"], \n",
    "    prompt_template=summary_prompt,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "key_words = KeywordExtractor(\n",
    "    keywords=5,\n",
    "    num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a42e1c-b5d1-4aa1-a65e-9a7c117bb63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_docs = documents[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b5b00-a0e5-4dc0-9f49-f46a75e74df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 88.40it/s]\n",
      "  0%|                                                                                                                                        | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:29<00:00,  8.96s/it]\n",
      "  0%|                                                                                                                                        | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:14<00:00,  1.48s/it]\n",
      "  0%|                                                                                                                                        | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, summary, key_words, qa_extractor]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=some_docs,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    "    # num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d17a3-22f1-4542-9c95-88734a251a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[2].__dict__['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a9cb1-ac81-4241-8708-c3ac79ae959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a31049-d0cb-41f3-ab86-76373a263ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import qdrant_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28f3224-4486-473b-8c47-dab936252c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19cfd5-7700-41c0-9768-9efe3fe33251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"WhereIsAI/UAE-Large-V1\",\n",
    "    tokenizer_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2113cb-555c-4b76-b182-4de2ffcebf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc1c4d-1ce6-4cc5-aa20-7e0f4bf0dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # uri=\"http://<host>:<port>\"\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe2f21-fe9e-4e90-bd57-d2220f2f54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc1ade-14f4-4b21-8265-0d0a688ef889",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ff868-0fa8-4178-910a-4d7cfc7207f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decilm_rag",
   "language": "python",
   "name": "decilm_rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
