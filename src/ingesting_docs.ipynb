{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleaning_utils\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from llama_index.core import  Document\n",
    "\n",
    "from llama_index.core.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    SummaryExtractor,\n",
    "    KeywordExtractor\n",
    "    \n",
    ")\n",
    "\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "\n",
    "from llama_index.core.node_parser import TokenTextSplitter, SentenceWindowNodeParser, SemanticSplitterNodeParser, SentenceSplitter\n",
    "from llama_index.core.schema import BaseNode, TextNode\n",
    "\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents_from_clean_text(cleaned_texts: List[Tuple[str, Dict]]) -> List[Document]:\n",
    "    documents = [Document(text=t, \n",
    "                          metadata=m, \n",
    "                          metadata_seperator=\"\\n\\n\", \n",
    "                          excluded_llm_metadata_keys=[\"file_name\",\n",
    "                                                      \"publication_date\", \n",
    "                                                      \"referenced_websites\", \n",
    "                                                      \"section_summary\", \n",
    "                                                      \"excerpt_keywords\",\n",
    "                                                      \"questions_this_excerpt_can_answer\"\n",
    "                                                     ]\n",
    "                         ) for (t, m) in cleaned_texts]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "cleaned_pdfs = cleaning_utils.clean_and_prepare_texts('../SuperMicro_Solution_Brief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    generate_kwargs={\"temperature\": 0.25, \n",
    "                     \"do_sample\": True, \n",
    "                     \"top_p\":0.80\n",
    "                     },\n",
    "    is_chat_model=True,\n",
    "    system_prompt = \"You are an AI assistant that follows instructions extremely well. Help as much as you can.\",\n",
    "    # query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"Deci/DeciLM-7B-instruct\",\n",
    "    model_name=\"Deci/DeciLM-7B-instruct\",\n",
    "    device_map=\"xpu\",\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": \"auto\",\n",
    "                  \"trust_remote_code\":True\n",
    "                 },\n",
    ")\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = \"\"\" Here is the context:\n",
    "{context_str}\n",
    "\n",
    "Given the contextual information, generate {num_questions} questions this context can provide \\\n",
    "specific answers about the products, software, hardware, and solutions mentioned in this document\\\n",
    "which are unlikely to be found elsewhere.\n",
    "\n",
    "Higher-level summaries of the surrounding context may be provided as well.  Try using these summaries to generate better questions that this context can answer.\"\"\"\n",
    "\n",
    "summary_prompt = \"\"\" Here is the content of the section:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "Provide a Summary of key topics, entities, products, software, hardware, and solutions discussed in this section.\n",
    "\n",
    "Summary: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", \n",
    "    chunk_size=256, \n",
    "    chunk_overlap=8\n",
    ")\n",
    "\n",
    "qa_extractor = QuestionsAnsweredExtractor(\n",
    "    questions=5, \n",
    "    prompt_template=qa_prompt,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "summary = SummaryExtractor(\n",
    "    summaries = [\"self\"], \n",
    "    prompt_template=summary_prompt,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "key_words = KeywordExtractor(\n",
    "    keywords=5,\n",
    "    num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_docs = documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter, summary, key_words, qa_extractor]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=some_docs,\n",
    "    in_place=True,\n",
    "    show_progress=True,\n",
    "    # num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[2].__dict__['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "import qdrant_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"WhereIsAI/UAE-Large-V1\",\n",
    "    tokenizer_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(\n",
    "    # you can use :memory: mode for fast and light-weight experiments,\n",
    "    # it does not require to have Qdrant deployed anywhere\n",
    "    # but requires qdrant-client >= 1.1.1\n",
    "    location=\":memory:\"\n",
    "    # otherwise set Qdrant instance address with:\n",
    "    # uri=\"http://<host>:<port>\"\n",
    "    # set API KEY for Qdrant Cloud\n",
    "    # api_key=\"<qdrant-api-key>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(client=client, collection_name=\"test\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
